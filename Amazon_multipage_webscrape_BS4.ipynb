{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357cc07c",
   "metadata": {},
   "source": [
    "# Amazon Multipage Webscrape in Beautiful Soup\n",
    "\n",
    "## This is for demonstration purposes only that yes we can programmatically scrape competitor price, product and rating data from Amazon, with the benefit of having no private API keys exposed on a public facing repository. But I caution Amazon aggressively monitors for and blacklists bots like this. \n",
    "\n",
    "## Better yet, the Amazon Product Advertising API using your seller account is the official API for data extraction on price, product reviews, etc. I can, and prefer, to work with these API's too!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2186dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf188740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_retry(url):\n",
    "    '''This is to evade annoying bot interceptors'''\n",
    "    ua= UserAgent()\n",
    "    uag_rand= ua.random\n",
    "    header={\n",
    "        'user-agent': uag_rand,\n",
    "        \"accept-language\": 'en-US'\n",
    "    }\n",
    "    isCaptcha = True\n",
    "    while isCaptcha:\n",
    "            page = requests.get(url, headers=header)\n",
    "            assert page.status_code==200\n",
    "            soup=BeautifulSoup(page.content, 'lxml')\n",
    "            if 'captcha' in str(soup):\n",
    "                uag_rand = ua.random\n",
    "                print(f'\\rbot detected..t=retrying..use new ua: {uag_rand}', end='', flush=True)\n",
    "            else: \n",
    "                print('bot bypassed')\n",
    "                return soup\n",
    "\n",
    "def search_keyword(kw):\n",
    "    count_page=0\n",
    "    count_asin=0\n",
    "    while True:\n",
    "        count_page +=1\n",
    "        url= f\"https://www.amazon.ca/s?k={kw}&page={count_page}\"\n",
    "        print(f'Getting page: {count_page}|{url}')\n",
    "        soup=get_soup_retry(url)\n",
    "        result= soup.find('div', attrs={'class':'s-main-slot s-result-list s-search-results sg-row'}).find_all(\"div\", attrs={'data-component-type': 's-search-result'})\n",
    "        \n",
    "        for ids in result:\n",
    "            count_asin +=1\n",
    "            asin = ids['data-asin']\n",
    "            url=f'https://www.amazon.com/dp/{asin}'\n",
    "            '''see below funtions for getting all the details on each item then write it to a df and to csv'''\n",
    "            list_result = get_details(url)\n",
    "            df=pd.DataFrame(list_result)\n",
    "            df.to_csv(f'result_amazon.csv', index=False)\n",
    "            \n",
    "            \n",
    "            print(f'{count_asin}.{url}')\n",
    "            \n",
    "        last_page = soup.find('li', attrs={'class':'a-disabled a-last'})\n",
    "        if not last_page:\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "\n",
    "result=[]\n",
    "\n",
    "def get_details(url):\n",
    "    soup = get_soup_retry(url)\n",
    "    today = datetime.date.today()\n",
    "    title = soup.find('span', attrs={'id':'productTitle'}).text.strip()\n",
    "    try:\n",
    "        price = soup.find('span', attrs={'class':\"a-offscreen\"}).text.strip() \n",
    "    except AttributeError:\n",
    "        price=''\n",
    "    try:\n",
    "        review_count = soup.find('span', attrs={\"id\": 'acrCustomerReviewText'}).text.strip()\n",
    "    except AttributeError:\n",
    "        review_count='' \n",
    "    try:\n",
    "        feature= soup.find(\"div\", attrs={'id':'feature-bullets'}).find('ul', attrs={'class':'a-unordered-list a-vertical a-spacing-mini'}).find_all(('li'))\n",
    "    \n",
    "        sv_feature=[]\n",
    "        for li in feature:\n",
    "            text=li.find('span', attrs={'class':'a-list-item'})\n",
    "            f=str(text.string).strip()\n",
    "            if \"None\" in f:\n",
    "                pass\n",
    "            else:\n",
    "                sv_feature.append(f)\n",
    "    except AttributeError:\n",
    "        sv_feature =''\n",
    "    try:\n",
    "        stock=soup.find('div', attrs={'id':\"availability\"}).find('span').text.strip()\n",
    "    except AttributeError:\n",
    "        stock ='' \n",
    "    try:\n",
    "        description=soup.find('div', attrs={'id':\"productDescription\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        description ='' \n",
    "        \n",
    "    print(today)\n",
    "    print(title)\n",
    "    print(price)\n",
    "    print(review_count)\n",
    "    print(sv_feature)\n",
    "    print(stock)\n",
    "    print(description)\n",
    "    \n",
    "    goal={\n",
    "        'datetime': today,\n",
    "        'title':title,\n",
    "        'price':price,\n",
    "        'review_count': review_count,\n",
    "        'features': sv_feature,\n",
    "        'stock': stock,\n",
    "        #'description': description\n",
    "    }\n",
    "    print(goal)\n",
    "    result.append(goal)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyword('NVIDIA+graphics+card&crid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e4ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    '''\n",
    "   Could be added below description in fun tion above. \n",
    "    \n",
    "    dt_img = soup.select('#imageBlock_feature_div> script:nth-child(2)')\n",
    "    try:\n",
    "        script_text = dt_img[0].text\n",
    "        json_str=re.search('{(.+)}', script_text)[0].replace(\"\\'\",'\"').replace(\"null\",'\"null\"')\n",
    "        json_obj=json.loads(json_str)\n",
    "        images_url=[]\n",
    "        for i in json_obj['initial']:\n",
    "            images_hires=i[\"hiRes\"]\n",
    "            images_large=i[\"large\"]\n",
    "            if images_hires is None:\n",
    "                images_url.append(images_large)\n",
    "            else:\n",
    "                images_url.append(images_hires)\n",
    "    except IndexErrorError:\n",
    "        images_url=''\n",
    "        \n",
    "\n",
    "and add:\n",
    "print(images_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
